import time
import json
import re
import os
from urllib.parse import urljoin, urlparse
from datetime import datetime
import random
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException
import requests
from bs4 import BeautifulSoup
import tempfile
import shutil

def detect_store_from_url(url):
    """Detecta automaticamente a loja baseado na URL"""
    url_lower = url.lower()
    
    if 'amazon' in url_lower:
        return 'Amazon'
    elif 'mercadolivre' in url_lower or 'mlstatic.com' in url_lower:
        return 'Mercado Livre'
    elif 'aliexpress' in url_lower:
        return 'AliExpress'
    elif 'americanas' in url_lower:
        return 'Americanas'
    elif 'casasbahia' in url_lower:
        return 'Casas Bahia'
    elif 'shopee' in url_lower:
        return 'Shopee'
    elif 'shein' in url_lower:
        return 'Shein'
    elif 'kabum' in url_lower:
        return 'Kabum'
    else:
        return 'Generic'

def setup_chrome_driver_heroku():
    """Configura√ß√£o do Chrome baseada em solu√ß√µes comprovadas do Heroku"""
    try:
        chrome_options = Options()
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes m√≠nimas que funcionam no Heroku
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        
        # SOLU√á√ÉO COMPROVADA: Usar diret√≥rio tempor√°rio √∫nico e limpo
        temp_dir = f"/tmp/chrome-{os.getpid()}-{int(time.time())}"
        os.makedirs(temp_dir, exist_ok=True)
        
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        chrome_options.add_argument(f'--data-path={temp_dir}')
        chrome_options.add_argument(f'--homedir={temp_dir}')
        chrome_options.add_argument(f'--disk-cache-dir={temp_dir}/cache')
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes de isolamento
        chrome_options.add_argument('--single-process')
        chrome_options.add_argument('--no-zygote')
        chrome_options.add_argument('--disable-setuid-sandbox')
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes de mem√≥ria
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--max_old_space_size=4096')
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes de rede
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--metrics-recording-only')
        chrome_options.add_argument('--no-report-upload')
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes de janela
        chrome_options.add_argument('--window-size=1366,768')
        chrome_options.add_argument('--start-maximized')
        
        # SOLU√á√ÉO COMPROVADA: User agent realista
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes experimentais
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # SOLU√á√ÉO COMPROVADA: Configura√ß√µes de seguran√ßa
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--allow-running-insecure-content')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        
        print(f"üîß Configurando Chrome com diret√≥rio tempor√°rio: {temp_dir}")
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("‚úÖ Chrome configurado com sucesso para Heroku")
        return driver, temp_dir
        
    except Exception as e:
        print(f"‚ùå Erro ao configurar Chrome para Heroku: {e}")
        return None, None

def cleanup_chrome_temp_dir(temp_dir):
    """Limpa o diret√≥rio tempor√°rio do Chrome"""
    if temp_dir and os.path.exists(temp_dir):
        try:
            shutil.rmtree(temp_dir)
            print(f"üßπ Diret√≥rio tempor√°rio limpo: {temp_dir}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao limpar diret√≥rio: {e}")

def extract_images_with_requests(url, store_name):
    """Fallback: Extrai imagens usando requests + BeautifulSoup"""
    try:
        print(f"üîÑ Fallback: Extraindo com requests + BeautifulSoup para {store_name}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        images_found = []
        
        # Buscar imagens <img>
        img_elements = soup.find_all('img')
        print(f"üì∏ Encontradas {len(img_elements)} imagens <img> via requests")
        
        for img in img_elements:
            try:
                src = img.get('src') or img.get('data-src') or img.get('data-lazy-src') or img.get('data-original')
                if src and is_main_product_image(src, img, store_name):
                    # Converter para URL absoluta se necess√°rio
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = urljoin(url, src)
                    elif not src.startswith('http'):
                        src = urljoin(url, src)
                    
                    image_info = create_image_info(src, img, store_name)
                    images_found.append(image_info)
            except Exception as e:
                continue
        
        # Buscar meta tags de imagem
        meta_images = soup.find_all('meta', property=re.compile(r'image', re.I))
        print(f"üì∏ Encontradas {len(meta_images)} meta tags de imagem via requests")
        
        for meta in meta_images:
            try:
                content = meta.get('content')
                if content and is_main_product_image(content, meta, store_name):
                    if content.startswith('//'):
                        content = 'https:' + content
                    elif content.startswith('/'):
                        content = urljoin(url, content)
                    elif not content.startswith('http'):
                        content = urljoin(url, content)
                    
                    image_info = create_image_info(content, meta, store_name)
                    images_found.append(image_info)
            except Exception as e:
                continue
        
        print(f"‚úÖ Total de imagens encontradas via requests: {len(images_found)}")
        return images_found
        
    except Exception as e:
        print(f"‚ùå Erro no fallback com requests: {str(e)}")
        return []

def extract_images_with_selenium(url, store_name):
    """Extrai imagens usando Selenium - vers√£o otimizada para Heroku"""
    driver = None
    temp_dir = None
    try:
        print(f"üöÄ Tentando extra√ß√£o com Selenium para {store_name}...")
        
        driver, temp_dir = setup_chrome_driver_heroku()
        if not driver:
            print("‚ùå Falha ao configurar Chrome, tentando fallback...")
            return None
        
        print("üåê Navegando para a URL...")
        driver.get(url)
        
        # Aguardar carregamento b√°sico
        time.sleep(random.uniform(3, 5))
        
        images_found = []
        
        # Buscar imagens <img>
        print("üîç Buscando imagens <img>...")
        img_elements = driver.find_elements(By.TAG_NAME, "img")
        print(f"üì∏ Encontradas {len(img_elements)} imagens <img>")
        
        for img in img_elements:
            try:
                src = img.get_attribute('src')
                if src and is_main_product_image(src, img, store_name):
                    image_info = create_image_info(src, img, store_name)
                    images_found.append(image_info)
            except Exception as e:
                continue
        
        # Buscar imagens com lazy loading
        print("üîç Buscando imagens com lazy loading...")
        lazy_images = driver.find_elements(By.CSS_SELECTOR, "img[data-src], img[data-lazy-src], img[data-original]")
        print(f"üì∏ Encontradas {len(lazy_images)} imagens lazy loading")
        
        for img in lazy_images:
            try:
                src = img.get_attribute('data-src') or img.get_attribute('data-lazy-src') or img.get_attribute('data-original')
                if src and is_main_product_image(src, img, store_name):
                    image_info = create_image_info(src, img, store_name)
                    images_found.append(image_info)
            except Exception as e:
                continue
        
        # Buscar meta tags
        print("üîç Buscando meta tags de imagem...")
        meta_images = driver.find_elements(By.CSS_SELECTOR, "meta[property*='image']")
        print(f"üì∏ Encontradas {len(meta_images)} meta tags de imagem")
        
        for meta in meta_images:
            try:
                content = meta.get_attribute('content')
                if content and is_main_product_image(content, meta, store_name):
                    image_info = create_image_info(content, meta, store_name)
                    images_found.append(image_info)
            except Exception as e:
                continue
        
        print(f"‚úÖ Selenium encontrou {len(images_found)} imagens")
        return images_found
        
    except Exception as e:
        print(f"‚ùå Erro no Selenium: {str(e)}")
        return None
    
    finally:
        if driver:
            try:
                print("üßπ Limpando recursos do Chrome...")
                driver.quit()
                print("‚úÖ Chrome fechado com sucesso")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao fechar Chrome: {e}")
        
        # Limpar diret√≥rio tempor√°rio
        if temp_dir:
            cleanup_chrome_temp_dir(temp_dir)

def is_main_product_image(src, element, store_name):
    """Filtro rigoroso para imagens principais de produto"""
    if not src:
        return False
    
    src_lower = src.lower()
    
    # Verificar se √© uma URL v√°lida (http/https)
    if not src.startswith(('http://', 'https://')):
        return False
    
    # Padr√µes espec√≠ficos para cada loja
    store_patterns = {
        'Amazon': [
            'images-na.ssl-images-amazon.com/images/',
            'images.amazon.com/images/',
            'm.media-amazon.com/images/',
            'product', 'prod'
        ],
        'Mercado Livre': [
            'http2.mlstatic.com/',
            'http2.mlstatic.com/D_',
            'mlstatic.com/',
            'product', 'produto'
        ],
        'AliExpress': [
            'ae01.alicdn.com/kf/',
            'ae02.alicdn.com/kf/',
            'ae03.alicdn.com/kf/',
            'ae04.alicdn.com/kf/',
            'product', 'prod', 'item'
        ],
        'Americanas': [
            'americanas.vtexassets.com/arquivos/ids/',
            'vtexassets.com/arquivos/ids/',
            'product', 'produto'
        ],
        'Casas Bahia': [
            'casasbahia.com.br/arquivos/ids/',
            'vtexassets.com/arquivos/ids/',
            'product', 'produto'
        ],
        'Shopee': [
            'shopee.com.br/arquivos/',
            'shopee.com.br/images/',
            'product', 'produto'
        ],
        'Shein': [
            'img.ltwebstatic.com/images3_ccc/',
            'sheinm.ltwebstatic.com/pwa_dist/images/',
            'product', 'produto'
        ],
        'Kabum': [
            'images.kabum.com.br/produtos/fotos/',
            'kabum.com.br/produtos/fotos/',
            'product', 'produto'
        ]
    }
    
    # Padr√µes de EXCLUS√ÉO rigorosos
    exclude_patterns = [
        'logo', 'icon', 'sprite', 'banner', 'ad', 'social',
        'favicon', 'avatar', 'profile', 'thumb', 'small',
        'transparent-pixel', 'grey-pixel', 'swatch-image',
        'cr-lightbox', 'review-image', 'community-reviews',
        'attach-accessory', 'button-icon', 'media-cheveron',
        'loading', 'placeholder', 'no-image', 'blank',
        'star', 'rating', 'review', 'coupon', 'discount',
        'arrow', 'chevron', 'close', 'menu', 'hamburger',
        'search', 'filter', 'sort', 'pagination',
        'vlibras', 'accessibility', 'a11y', 'libras'
    ]
    
    # Verificar exclus√µes primeiro
    for pattern in exclude_patterns:
        if pattern in src_lower:
            return False
    
    # Verificar se √© uma imagem muito pequena (provavelmente √≠cone)
    if any(size in src_lower for size in ['16x16', '24x24', '32x32', '48x48', '64x64']):
        return False
    
    # Verificar padr√µes da loja espec√≠fica
    patterns = store_patterns.get(store_name, [])
    for pattern in patterns:
        if pattern in src_lower:
            return True
    
    # Verificar extens√µes de imagem v√°lidas
    if not any(src_lower.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
        return False
    
    # Verificar se parece uma URL de produto (n√£o muito gen√©rica)
    if 'http' in src_lower:
        product_indicators = ['product', 'prod', 'item', 'image', 'photo', 'pic', 'gallery']
        if not any(indicator in src_lower for indicator in product_indicators):
            return False
    
    return True

def get_image_dimensions(url):
    """Obt√©m o tamanho real da imagem via HEAD request"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
        
        if response.status_code == 200:
            content_length = response.headers.get('content-length', 0)
            return int(content_length) if content_length else 0
        return 0
    except:
        return 0

def create_image_info(src, element, store_name):
    """Cria informa√ß√µes da imagem com dados para ordena√ß√£o por qualidade"""
    try:
        file_size = get_image_dimensions(src)
        
        # Extrair atributos baseado no tipo de elemento
        if hasattr(element, 'get'):
            # Elemento BeautifulSoup
            alt = element.get('alt', '')
            title = element.get('title', '')
            width = element.get('width', '')
            height = element.get('height', '')
            class_attr = element.get('class', '')
            id_attr = element.get('id', '')
            tag_name = element.name
        else:
            # Elemento Selenium
            alt = element.get_attribute('alt') or ''
            title = element.get_attribute('title') or ''
            width = element.get_attribute('width') or ''
            height = element.get_attribute('height') or ''
            class_attr = element.get_attribute('class') or ''
            id_attr = element.get_attribute('id') or ''
            tag_name = element.tag_name
        
        return {
            'url': src,
            'alt': alt,
            'title': title,
            'width': width,
            'height': height,
            'class': class_attr,
            'id': id_attr,
            'element_type': tag_name,
            'file_size_bytes': file_size,
            'quality_score': 0
        }
    except:
        return {
            'url': src,
            'alt': '',
            'title': '',
            'width': '',
            'height': '',
            'class': '',
            'id': '',
            'element_type': 'unknown',
            'file_size_bytes': 0,
            'quality_score': 0
        }

def calculate_quality_score(image_info, store_name):
    """Calcula score de qualidade baseado em resolu√ß√£o e tamanho real"""
    score = 0
    url = image_info['url'].lower()
    
    # Pontuar por tamanho do arquivo (mais preciso que dimens√µes HTML)
    if image_info['file_size_bytes'] > 0:
        # Normalizar tamanho: 1MB = 100 pontos
        score += (image_info['file_size_bytes'] / 1024 / 1024) * 100
    
    # Pontuar por dimens√µes HTML (se dispon√≠vel)
    if image_info['width'] and image_info['height']:
        try:
            width = int(image_info['width'])
            height = int(image_info['height'])
            # Normalizar: 1000x1000 = 50 pontos
            score += (width * height) / 20000
        except:
            pass
    
    # Pontuar por extens√£o de arquivo
    if url.endswith('.jpg') or url.endswith('.jpeg'):
        score += 15
    elif url.endswith('.png'):
        score += 12
    elif url.endswith('.webp'):
        score += 10
    
    # Pontuar por padr√µes espec√≠ficos da loja
    if store_name == 'Amazon':
        if 'images-na.ssl-images-amazon.com/images/' in url:
            score += 30
        if 'm.media-amazon.com/images/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    elif store_name == 'Mercado Livre':
        if 'http2.mlstatic.com/D_' in url:
            score += 30
        if 'mlstatic.com/' in url:
            score += 25
        if 'product' in url:
            score += 20
    
    elif store_name == 'AliExpress':
        if 'ae01.alicdn.com/kf/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    elif store_name == 'Americanas':
        if 'americanas.vtexassets.com/arquivos/ids/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    elif store_name == 'Casas Bahia':
        if 'casasbahia.com.br/arquivos/ids/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    elif store_name == 'Shopee':
        if 'shopee.com.br/arquivos/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    elif store_name == 'Shein':
        if 'img.ltwebstatic.com/images3_ccc/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    elif store_name == 'Kabum':
        if 'images.kabum.com.br/produtos/fotos/' in url:
            score += 30
        if 'product' in url:
            score += 25
    
    # Pontuar por padr√µes de qualidade na URL
    if any(term in url for term in ['high', 'large', 'original', 'full', 'hd', '4k', '1280', '1920']):
        score += 25
    
    # Pontuar por alt text significativo
    if len(image_info['alt']) > 20:
        score += 15
    
    return score

def get_base_image_url(url):
    """Remove par√¢metros de URL para identificar imagens duplicadas"""
    base_url = url.split('?')[0].split('#')[0]
    
    # Remover par√¢metros de tamanho espec√≠ficos
    base_url = re.sub(r'\._AC_[^_]+_\.', '._AC_.', base_url)
    base_url = re.sub(r'\._SS\d+_\.', '._SS_.', base_url)
    base_url = re.sub(r'\._SY\d+_\.', '._SY_.', base_url)
    
    return base_url

def extract_product_images(url, store_name=None):
    """
    Fun√ß√£o principal para extrair imagens de produto com estrat√©gia h√≠brida
    
    Args:
        url (str): URL do produto
        store_name (str, optional): Nome da loja (ser√° detectado automaticamente se n√£o fornecido)
    
    Returns:
        dict: Dicion√°rio com informa√ß√µes das imagens extra√≠das
    """
    try:
        # Detectar loja automaticamente se n√£o fornecida
        if not store_name:
            store_name = detect_store_from_url(url)
        
        print(f"Iniciando extra√ß√£o para {store_name}: {url}")
        
        # Estrat√©gia 1: Tentar Selenium primeiro (mais robusto)
        images = extract_images_with_selenium(url, store_name)
        
        # Estrat√©gia 2: Se Selenium falhar, usar requests + BeautifulSoup
        if not images:
            print("üîÑ Selenium falhou, tentando fallback com requests...")
            images = extract_images_with_requests(url, store_name)
        
        if not images:
            print("‚ùå Nenhuma imagem encontrada com nenhuma estrat√©gia")
            return None
        
        print(f"Encontradas {len(images)} imagens de produto")
        
        # Calcular scores de qualidade
        for img in images:
            img['quality_score'] = calculate_quality_score(img, store_name)
        
        # Ordenar por qualidade (melhor para pior)
        images.sort(key=lambda x: x['quality_score'], reverse=True)
        
        # Remover duplicatas baseado na URL base
        unique_images = []
        seen_urls = set()
        
        for img in images:
            base_url = get_base_image_url(img['url'])
            if base_url not in seen_urls:
                seen_urls.add(base_url)
                unique_images.append(img)
        
        print(f"Ap√≥s remo√ß√£o de duplicatas: {len(unique_images)} imagens √∫nicas")
        
        # Determinar m√©todo de extra√ß√£o usado
        extraction_method = 'selenium_headless' if images else 'requests_beautifulsoup'
        
        return {
            'store_name': store_name,
            'url': url,
            'extraction_date': datetime.now().isoformat(),
            'total_images_found': len(unique_images),
            'extraction_method': extraction_method,
            'images': unique_images
        }
        
    except Exception as e:
        print(f"Erro na extra√ß√£o: {str(e)}")
        raise e
